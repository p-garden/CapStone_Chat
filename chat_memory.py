# chat_with_memory.py

import os
import json
from datetime import datetime
from pathlib import Path

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# === ì‚¬ìš©ì ì„¤ì • ===
USER_ID = "user_001"
VECTOR_DIR = Path("vector_store")
VECTOR_DIR.mkdir(exist_ok=True)

client = OpenAI()
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# === íŒŒì¼ ê²½ë¡œ ===
faiss_path = VECTOR_DIR / f"{USER_ID}.faiss"
meta_path = VECTOR_DIR / f"{USER_ID}.json"

# === ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ===
if faiss_path.exists() and meta_path.exists():
    index = faiss.read_index(str(faiss_path))
    with open(meta_path, "r") as f:
        metadata = json.load(f)
else:
    index = faiss.IndexFlatL2(384)
    metadata = []

# === ë²¡í„° ì €ì¥ í•¨ìˆ˜ ===
def save_summary(text):
    vec = embed_model.encode([text])
    index.add(np.array(vec).astype("float32"))
    metadata.append({"summary": text, "timestamp": datetime.now().isoformat()})
    faiss.write_index(index, str(faiss_path))
    with open(meta_path, "w") as f:
        json.dump(metadata, f, indent=2)

# === ê²€ìƒ‰ í•¨ìˆ˜ ===
def retrieve_summaries(query, top_k=3):
    if len(metadata) == 0:
        return []
    q_vec = embed_model.encode([query])
    D, I = index.search(np.array(q_vec).astype("float32"), top_k)
    return [metadata[i]["summary"] for i in I[0] if i < len(metadata)]

# === í˜ë¥´ì†Œë‚˜ ì„ íƒ ===
personality_prompts = {
    "ë‹¤ì •í•œ ì¹œêµ¬": "...",  # í”„ë¡¬í”„íŠ¸ ìƒëµ (ìœ„ ì½”ë“œ ë‚´ìš© ê·¸ëŒ€ë¡œ)
    "í˜„ì‹¤ì ì¸ ì„ ë°°": "...",
    "ì´ì„±ì ì¸ ì¡°ì–¸ê°€": "..."
}

print("ğŸ‘¥ ìƒë‹´ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì„¸ìš”:")
print("1. ë‹¤ì •í•œ ì¹œêµ¬")
print("2. í˜„ì‹¤ì ì¸ ì„ ë°°")
print("3. ì´ì„±ì ì¸ ì¡°ì–¸ê°€")
choice = input("ğŸ‘‰ ì„ íƒ (1 ~ 3): ")

types = {"1": "ë‹¤ì •í•œ ì¹œêµ¬", "2": "í˜„ì‹¤ì ì¸ ì„ ë°°", "3": "ì´ì„±ì ì¸ ì¡°ì–¸ê°€"}
persona_type = types.get(choice, "ë‹¤ì •í•œ ì¹œêµ¬")

# === system prompt êµ¬ì„± ===
recent_context = retrieve_summaries("ìµœê·¼ ê°ì •, ìˆ˜ë©´, ìŠ¤íŠ¸ë ˆìŠ¤ ë“±")
context_prompt = "\n".join([f"- {item}" for item in recent_context])
system_content = f"""
ì´ì „ì— ì‚¬ìš©ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ë ¥ì´ ìˆìŠµë‹ˆë‹¤:
{context_prompt}

ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”ë¥¼ ì´ì–´ê°€ ì£¼ì„¸ìš”.

{personality_prompts[persona_type]}
"""

messages = [{"role": "system", "content": system_content}]

print(f"\nğŸ’¬ {persona_type}ì™€ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")

while True:
    user_input = input("ğŸ‘¤ ë‚˜: ")
    if user_input.lower() == "exit":
        print("ğŸ‘‹ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    messages.append({"role": "user", "content": user_input})

    response = client.chat.completions.create(
        model="gpt-4o-mini-2024-07-18",
        messages=messages,
        temperature=0.7,
        max_tokens=512,
    )
    reply = response.choices[0].message.content
    print(f"ğŸ¤– {persona_type}: {reply}")
    messages.append({"role": "assistant", "content": reply})

    # ëŒ€í™” ë‚´ìš© ìš”ì•½í•´ì„œ ì €ì¥ (ê°„ë‹¨íˆ input + replyë¡œ ì €ì¥)
    summary_to_save = f"User: {user_input}\nBot: {reply}"
    save_summary(summary_to_save)