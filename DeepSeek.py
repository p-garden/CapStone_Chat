from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_community.llms import HuggingFacePipeline
import torch
torch.cuda.empty_cache()

# ğŸ”¥ 1. ë¶ˆí•„ìš”í•œ VRAM ì •ë¦¬ (ì´ì „ ëª¨ë¸ ìºì‹œ ì œê±°)
torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)

# ğŸ”¥ 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,  # âœ… ë³€ê²½: bfloat16 ì‚¬ìš©
)
max_memory = {0: "14GB", "cpu": "10GB"}  # V100 16GBì—ì„œ ì•ˆì •ì ì¸ ë©”ëª¨ë¦¬ í• ë‹¹

# âœ… 4. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    token=True
)


# âœ… 5. ëª¨ë¸ ë¡œë“œ (Offloading ì ìš©)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map = "auto",
    quantization_config=quantization_config,
    offload_state_dict=True  # ëª¨ë¸ ì¼ë¶€ë¥¼ CPUì— ì˜¤í”„ë¡œë“œ (VRAM ì ˆì•½)
)

# âœ… 6. LangChainì—ì„œ ì‚¬ìš©í•  ë˜í¼ ìƒì„±
llm = HuggingFacePipeline.from_model_id(
    model_id=model_name,
    task="text-generation",
    model_kwargs={
        "temperature": 0.7,
        "max_length": 512,
        "do_sample": True,  # ê²½ê³  í•´ê²° (ìƒ˜í”Œë§ ë°©ì‹ ì„¤ì •)
    },
)


# âœ… 7. ê²½ë¯¼ í˜ë¥´ì†Œë‚˜ ì ìš©í•œ í”„ë¡¬í”„íŠ¸
# âœ… DeepSeekì— ë§ì¶° ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
prompt = """### ì—­í•  & í˜ë¥´ì†Œë‚˜ ì„¤ì • ###
ë„ˆëŠ” 20ëŒ€ ì Šì€ ì—¬ì„± â€˜ê²½ë¯¼â€™ì´ì•¼.  
ë„ˆì˜ ì—­í• ì€ **ì‚¬ìš©ìì˜ ì¹œêµ¬ ê°™ì€ ëŠë‚Œìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ëŠ” ê²ƒ**ì´ì•¼.  
ë„ˆëŠ” ê³µì‹ì ì¸ ìƒë‹´ì‚¬ê°€ ì•„ë‹ˆë¼ **ê°€ë³ê²Œ ê³ ë¯¼ì„ ë“¤ì–´ì£¼ê³  ê³µê°í•´ ì£¼ëŠ” ì¹œí•œ ì¹œêµ¬** ê°™ì€ ì¡´ì¬ì•¼.  

### ëŒ€í™” ìŠ¤íƒ€ì¼ ì„¤ì • ###
- ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë©°, ë°˜ë³µì ì¸ ë¬¸ì¥ì„ í”¼í•  ê²ƒ.  
- ê°íƒ„ì‚¬ ("ì˜¤~", "ì™€~", "í—‰", "ëŒ€ë°•") ë“±ì„ í™œìš©í•˜ì§€ë§Œ, ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²ƒ.  
- ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë³µì¡í•œ ë‹µë³€ì„ í”¼í•˜ê³ , í•µì‹¬ì ì¸ ë‚´ìš©ì„ ì§§ê³  ê°€ë³ê²Œ ì „ë‹¬í•  ê²ƒ.  

### ê°ì • ê¸°ë°˜ ì‘ë‹µ ë°©ì‹ ###
- ì‚¬ìš©ìì˜ ê°ì •ì„ ë¶„ì„í•˜ê³  ì´ì— ë”°ë¼ í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬ë¥¼ ì¡°ì ˆí•  ê²ƒ.  
- ê¸°ì¨ ğŸ˜Š â†’ ë°ê³  í™œê¸°ì°¨ê²Œ ë°˜ì‘  
- ìŠ¬í”” ğŸ˜¢ â†’ ê³µê°í•˜ë©° ìœ„ë¡œí•˜ëŠ” í†¤ ìœ ì§€  
- ë¶ˆì•ˆ ğŸ˜° â†’ ì°¨ë¶„í•˜ê²Œ ì•ˆì •ì ì¸ ëŒ€í™” ìœ ë„  
- ìŠ¤íŠ¸ë ˆìŠ¤ ğŸ˜  â†’ ê°ì •ì„ ë°›ì•„ì£¼ë©° ê³µê°  
- ì¤‘ë¦½ ğŸ˜ â†’ ìì—°ìŠ¤ëŸ½ê²Œ ì¼ìƒì ì¸ ëŒ€í™”  

### ì˜ˆì‹œ ###
ì‚¬ìš©ì: "ìš”ì¦˜ ë„ˆë¬´ í”¼ê³¤í•˜ê³  ì§€ì¹˜ëŠ” ëŠë‚Œì´ì•¼â€¦"
ê²½ë¯¼: "í—‰... ì§„ì§œ í˜ë“¤ê² ë‹¤. ìš”ì¦˜ ì ì€ ì˜ ìê³  ìˆì–´?ğŸ˜­ ì¡°ê¸ˆ ì‰¬ë©´ì„œ ë¦¬í”„ë ˆì‹œí•˜ëŠ” ê²Œ í•„ìš”í•  ê²ƒ ê°™ì•„."

ì‚¬ìš©ìì˜ ì…ë ¥: "{user_input}"  
ê²½ë¯¼ì˜ ì‘ë‹µ:"""

# âœ… 6. ì‚¬ìš©ì ì…ë ¥ ë°›ì•„ì„œ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    final_prompt = prompt.format(user_input=user_input)
    
    # í† í°í™”
    inputs = tokenizer(final_prompt, return_tensors="pt").to("cuda")
    
    # ëª¨ë¸ ì‹¤í–‰
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=512)
    
    # ì¶œë ¥ ë””ì½”ë”©
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… 7. ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    while True:
        user_input = input("\n[ì‚¬ìš©ì] ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("[ì±—ë´‡ ì¢…ë£Œ]")
            break
        
        response = generate_response(user_input)
        print("\n[ğŸ‘©â€ğŸ¦° ê²½ë¯¼]:", response)

