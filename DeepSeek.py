from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_community.llms import HuggingFacePipeline
import torch

# ğŸ”¥ 1. ë¶ˆí•„ìš”í•œ VRAM ì •ë¦¬ (ì´ì „ ëª¨ë¸ ìºì‹œ ì œê±°)
torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)

# ğŸ”¥ 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "DeepSeek-R1-Distill-Qwen-7B"

# âœ… 3. 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½ + ìµœì í™”)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4-bit ì–‘ìí™” ì ìš©
    bnb_4bit_use_double_quant=True,  # 2ì¤‘ ì–‘ìí™” (RAM ì ˆì•½)
    bnb_4bit_quant_type="nf4",  # NF4 ì–‘ìí™” ë°©ì‹ (ë” ë‚˜ì€ ì„±ëŠ¥)
    bnb_4bit_compute_dtype=torch.float16,  # 16-bit ì—°ì‚° ìœ ì§€
)

# âœ… 4. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    token=True  # ìµœì‹  ë°©ì‹ ì ìš©
)

# âœ… 5. ëª¨ë¸ ë¡œë“œ (Offloading ì ìš©)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",  # GPU+CPU ìë™ í• ë‹¹ (Offloading)
    quantization_config=quantization_config,
    offload_folder="offload"  # ì¼ë¶€ CPUë¡œ ë¡œë“œ (Offloading)
)

# âœ… 6. LangChainì—ì„œ ì‚¬ìš©í•  ë˜í¼ ìƒì„±
llm = HuggingFacePipeline.from_model_id(
    model_id=model_name,
    task="text-generation",
    model_kwargs={
        "temperature": 0.7,
        "max_length": 512,
        "do_sample": True,  # ê²½ê³  í•´ê²° (ìƒ˜í”Œë§ ë°©ì‹ ì„¤ì •)
    },
)

# âœ… 7. í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
prompt = "ì‹¬ë¦¬ ìƒë‹´ì„ ìœ„í•œ ì¡°ì–¸ì„ ì œê³µí•´ì¤˜."
response = llm(prompt)

print("\nğŸ“ ìƒë‹´ ì±—ë´‡ ì‘ë‹µ:\n", response)