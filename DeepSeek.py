from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_community.llms import HuggingFacePipeline
import torch
torch.cuda.empty_cache()

# ğŸ”¥ 1. ë¶ˆí•„ìš”í•œ VRAM ì •ë¦¬ (ì´ì „ ëª¨ë¸ ìºì‹œ ì œê±°)
torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)

# ğŸ”¥ 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,  # âœ… ë³€ê²½: bfloat16 ì‚¬ìš©
)
max_memory = {0: "14GB", "cpu": "10GB"}  # V100 16GBì—ì„œ ì•ˆì •ì ì¸ ë©”ëª¨ë¦¬ í• ë‹¹

# âœ… 4. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    token=True
)


# âœ… 5. ëª¨ë¸ ë¡œë“œ (Offloading ì ìš©)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map = "auto",
    quantization_config=quantization_config,
    offload_state_dict=True  # ëª¨ë¸ ì¼ë¶€ë¥¼ CPUì— ì˜¤í”„ë¡œë“œ (VRAM ì ˆì•½)
)

# âœ… 6. LangChainì—ì„œ ì‚¬ìš©í•  ë˜í¼ ìƒì„±
llm = HuggingFacePipeline.from_model_id(
    model_id=model_name,
    task="text-generation",
    model_kwargs={
        "temperature": 0.7,
        "max_length": 512,
        "do_sample": True,  # ê²½ê³  í•´ê²° (ìƒ˜í”Œë§ ë°©ì‹ ì„¤ì •)
    },
)

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
inputs = tokenizer("ì‹¬ë¦¬ ìƒë‹´ì„ ìœ„í•œ ì¡°ì–¸ì„ ì œê³µí•´ì¤˜.", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_length=512)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))