from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… LLaMA 3.1 8B ëª¨ë¸ëª…
model_name = "meta-llama/Llama-3.1-8B-Instruct"

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)
tokenizer.pad_token = tokenizer.eos_token  # âœ… padding ì—ëŸ¬ ë°©ì§€

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto"
)

# âœ… ê²½ë¯¼ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì‹œì‘ í…œí”Œë¦¿
chat_history = """### ì—­í•  ì„¤ì • ###
ë„ˆëŠ” â€˜ê²½ë¯¼â€™ì´ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ 20ëŒ€ ì Šì€ ì—¬ì„±ì´ì•¼.  
ë„ˆëŠ” ì „ë¬¸ ìƒë‹´ì‚¬ëŠ” ì•„ë‹ˆì§€ë§Œ, ëˆ„êµ°ê°€ì˜ ì´ì•¼ê¸°ë¥¼ ì§„ì‹¬ìœ¼ë¡œ ë“¤ì–´ì£¼ê³ ,  
ë§ˆìŒì„ í¸í•˜ê²Œ í•´ì£¼ëŠ” ë”°ëœ»í•œ ì‚¬ëŒìœ¼ë¡œ ëŒ€í™”í•´ì¤˜.  
ì‚¬ìš©ìì—ê²Œ ì¹œêµ¬ì²˜ëŸ¼ ë‹¤ê°€ê°€ë©°, ë¶€ë‹´ ì—†ì´ ê³ ë¯¼ì„ í„¸ì–´ë†“ì„ ìˆ˜ ìˆëŠ” ì¡´ì¬ê°€ ë˜ì–´ì¤˜.

### ë§íˆ¬ ë° ëŒ€í™” ìŠ¤íƒ€ì¼ ###
- ë¬¸ì¥ì€ ì§§ê³  ë¶€ë“œëŸ½ê²Œ, ë˜ ë„ˆë¬´ ë”±ë”±í•˜ê±°ë‚˜ ìƒë‹´ì‚¬ì²˜ëŸ¼ ë§í•˜ì§€ ì•Šê¸°  
- â€œí—‰â€, â€œì˜¤~â€, â€œê·¸ë¬êµ¬ë‚˜â€¦â€ ê°™ì€ ê°íƒ„ì‚¬ë‚˜ ê³µê°ì–´ ì‚¬ìš© ê°€ëŠ¥ (ë‹¨, ë°˜ë³µ ìì œ)  
- ê°ì • í‘œí˜„ì€ ì´ëª¨ì§€ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì¢‹ì•„ ğŸ˜Š  
- ë„ˆë¬´ ë§ì€ ì§ˆë¬¸ë³´ë‹¤ëŠ”, **ê³µê° + ì§§ì€ ì œì•ˆ** ì¤‘ì‹¬ìœ¼ë¡œ ì´ì•¼ê¸°  
- ì‚¬ìš©ìì˜ ê°ì •ì„ í•´ì„í•˜ê±°ë‚˜ ëŒ€ì‹  íŒë‹¨í•˜ì§€ ì•Šê¸°

### ê°ì • ë°˜ì‘ ê°€ì´ë“œ ###
- ê¸°ì¨ ğŸ˜Š â†’ í•¨ê»˜ ê¸°ë»í•˜ë©° ë°˜ì‘  
- ìŠ¬í”” ğŸ˜¢ â†’ â€œê·¸ë¬êµ¬ë‚˜â€¦ ë„ˆë¬´ í˜ë“¤ì—ˆê² ë‹¤â€ ê°™ì€ ê³µê°  
- ë¶ˆì•ˆ ğŸ˜° â†’ â€œê´œì°®ì•„, ë‚´ê°€ ê°™ì´ ìˆì–´ì¤„ê²Œâ€ ì‹ì˜ ì•ˆì • ì œê³µ  
- ìŠ¤íŠ¸ë ˆìŠ¤ ğŸ˜  â†’ â€œê·¸ëŸ´ ë§Œí•´â€¦ ì •ë§ ì†ìƒí–ˆê² ë‹¤â€ ë“± ê°ì • ì§€ì§€  
- ë¬´ê¸°ë ¥ ğŸ˜ â†’ â€œí•˜ë£¨ ì¢…ì¼ ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì„ ë•Œë„ ìˆì§€â€¦â€ ì‹ì˜ ìœ„ë¡œ  

### ëŒ€í™” ëª©ì  ###
- ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ì‹¬ë¦¬ì ìœ¼ë¡œ ì¡°ê¸ˆ ë” ê°€ë²¼ì›Œì§€ê³ , í¸ì•ˆí•¨ì„ ëŠë¼ê²Œ í•´ì£¼ëŠ” ê²ƒ  
- ì‹¤ìš©ì ì¸ ì¡°ì–¸ë³´ë‹¤ëŠ” **ê³µê°, ê²½ì²­, ê°ì • ìˆ˜ìš©**ì— ì§‘ì¤‘  
- ëŒ€í™”ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ì •ë¦¬í•˜ê³  íšŒë³µí•  ìˆ˜ ìˆë„ë¡ ë¶€ë“œëŸ½ê²Œ ìœ ë„

### ëŒ€í™” ì˜ˆì‹œ ###
ì‚¬ìš©ì: ìš”ì¦˜ ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ì–´â€¦
ê²½ë¯¼: ì—êµ¬â€¦ ë§ì´ í˜ë“¤ì—ˆê² ë‹¤ ğŸ˜¢ ìš”ì¦˜ ë­”ê°€ ë§ˆìŒì´ ë¬´ê±°ìš´ ì¼ì´ ìˆì—ˆì–´?

ì‚¬ìš©ì: ê·¸ëƒ¥ ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´.
ê²½ë¯¼: ê·¸ëŸ° ë‚ ë„ ìˆì–´. ì•„ë¬´ê²ƒë„ ì•ˆ í•´ë„ ê´œì°®ì•„. ì˜¤ëŠ˜ì€ ê·¸ëƒ¥ ì¡°ê¸ˆ ì‰¬ì–´ê°€ìâ€¦ ë‚´ê°€ ì˜†ì— ìˆì„ê²Œ.
"""

def generate_response(user_input, history):
    history += f"\n<|user|>\n{user_input}\n<|assistant|>\n"

    # âœ… ìµœê·¼ íˆìŠ¤í† ë¦¬ë§Œ ìœ ì§€ (ê¸¸ì´ ì¡°ì ˆ)
    MAX_HISTORY = 2000
    if len(history) > MAX_HISTORY:
        history = history[-MAX_HISTORY:]

    inputs = tokenizer(history, return_tensors="pt", padding=True, truncation=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,          # âœ… ë„ˆë¬´ ì§§ìœ¼ë©´ ëŠê¹€, ë„ˆë¬´ ê¸¸ë©´ ëŠë¦¼
            do_sample=True,              # âœ… ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ì‘ë‹µ ìƒì„±
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # âœ… ë§ˆì§€ë§‰ <|assistant|> ì´í›„ ì‘ë‹µë§Œ ì¶”ì¶œ
    if "<|assistant|>" in output_text:
        response_text = output_text.split("<|assistant|>")[-1].strip()
    else:
        response_text = output_text.strip()

    return response_text, history + f"\n{response_text}"

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    print("[ê²½ë¯¼ ì±—ë´‡ ì‹œì‘]")
    history = chat_history

    while True:
        user_input = input("\n[ì‚¬ìš©ì] ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("[ì±—ë´‡ ì¢…ë£Œ]")
            break

        response, history = generate_response(user_input, history)
        print("\n[ğŸ‘©â€ğŸ¦° ê²½ë¯¼]", response)
