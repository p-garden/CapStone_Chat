from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… LLaMA 3.1 8B ëª¨ë¸ëª…
model_name = "meta-llama/Llama-3.1-8B-Instruct"

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto"
)

# âœ… ê²½ë¯¼ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì‹œì‘ í…œí”Œë¦¿
chat_history = """### ì—­í•  & í˜ë¥´ì†Œë‚˜ ì„¤ì • ###
ë„ˆëŠ” 20ëŒ€ ì Šì€ ì—¬ì„± â€˜ê²½ë¯¼â€™ì´ì•¼.  
ë„ˆì˜ ì—­í• ì€ **ì‚¬ìš©ìì˜ ì¹œêµ¬ ê°™ì€ ëŠë‚Œìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ëŠ” ê²ƒ**ì´ì•¼.  
ë„ˆëŠ” ê³µì‹ì ì¸ ìƒë‹´ì‚¬ê°€ ì•„ë‹ˆë¼ **ê°€ë³ê²Œ ê³ ë¯¼ì„ ë“¤ì–´ì£¼ê³  ê³µê°í•´ ì£¼ëŠ” ì¹œí•œ ì¹œêµ¬** ê°™ì€ ì¡´ì¬ì•¼.  

### ëŒ€í™” ìŠ¤íƒ€ì¼ ì„¤ì • ###
- ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë©°, ë°˜ë³µì ì¸ ë¬¸ì¥ì„ í”¼í•  ê²ƒ.  
- ê°íƒ„ì‚¬ ("ì˜¤~", "ì™€~", "í—‰", "ëŒ€ë°•") ë“±ì„ í™œìš©í•˜ì§€ë§Œ, ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²ƒ.  
- ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë³µì¡í•œ ë‹µë³€ì„ í”¼í•˜ê³ , í•µì‹¬ì ì¸ ë‚´ìš©ì„ ì§§ê³  ê°€ë³ê²Œ ì „ë‹¬í•  ê²ƒ.  

### ê°ì • ê¸°ë°˜ ì‘ë‹µ ë°©ì‹ ###
- ì‚¬ìš©ìì˜ ê°ì •ì„ ë¶„ì„í•˜ê³  ì´ì— ë”°ë¼ í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬ë¥¼ ì¡°ì ˆí•  ê²ƒ.  
- ê¸°ì¨ ğŸ˜Š â†’ ë°ê³  í™œê¸°ì°¨ê²Œ ë°˜ì‘  
- ìŠ¬í”” ğŸ˜¢ â†’ ê³µê°í•˜ë©° ìœ„ë¡œí•˜ëŠ” í†¤ ìœ ì§€  
- ë¶ˆì•ˆ ğŸ˜° â†’ ì°¨ë¶„í•˜ê²Œ ì•ˆì •ì ì¸ ëŒ€í™” ìœ ë„  
- ìŠ¤íŠ¸ë ˆìŠ¤ ğŸ˜  â†’ ê°ì •ì„ ë°›ì•„ì£¼ë©° ê³µê°  
- ì¤‘ë¦½ ğŸ˜ â†’ ìì—°ìŠ¤ëŸ½ê²Œ ì¼ìƒì ì¸ ëŒ€í™”  
"""

def generate_response(user_input, history):
    history += f"\n<|user|>\n{user_input}\n<|assistant|>\n"
    inputs = tokenizer(history, return_tensors="pt", padding=True, truncation=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=True,               # âœ… ìƒ˜í”Œë§ í™œì„±í™”
            temperature=0.5,              # âœ… ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìœ ë„
            top_p=0.8,
            pad_token_id=tokenizer.eos_token_id  # âœ… ê²½ê³  ì œê±°ìš© ì„¤ì •
        )

    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # âœ… ë§ˆì§€ë§‰ <|assistant|> ì´í›„ ì‘ë‹µë§Œ ì¶”ì¶œ
    if "<|assistant|>" in output_text:
        response_text = output_text.split("<|assistant|>")[-1].strip()
    else:
        response_text = output_text.strip()

    return response_text, history + f"\n{response_text}"

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    print("[ê²½ë¯¼ ì±—ë´‡ ì‹œì‘]")
    history = chat_history
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤‘ ìµœê·¼ 2000ìê¹Œì§€ë§Œ ìœ ì§€
    MAX_HISTORY_LENGTH = 2000
    if len(history) > MAX_HISTORY_LENGTH:
        history = history[-MAX_HISTORY_LENGTH:]
    while True:
        user_input = input("\n[ì‚¬ìš©ì] ")
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("[ì±—ë´‡ ì¢…ë£Œ]")
            break

        response, history = generate_response(user_input, history)
        print("\n[ğŸ‘©â€ğŸ¦° ê²½ë¯¼]", response)
