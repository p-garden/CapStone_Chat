from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from agents.subllm_agent import SubLLMAgent
from prompts.prompt_builder import build_prompt_with_strategies
import os
import re
class CounselorAgent:
    def __init__(self, client_info, persona_type, model_name="gpt-4o-mini", temperature=0.7):
        self.client_info = client_info
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.subllm = SubLLMAgent()
        # Load persona prompt based on persona_type
        persona_path = f"prompts/{persona_type}.txt"
        with open(persona_path, "r", encoding="utf-8") as f:
            self.persona_prompt = f.read()

        self.prompt_template = self.load_prompt_template()

    def load_prompt_template(self):
        with open("prompts/counselor_prompt.txt", "r", encoding="utf-8") as f:
            return f.read()

    # ê°ì •ê³¼ ì¸ì§€ ì™œê³¡ì— ë§ëŠ” ì „ëµ ê²°í•©
        return f"{emotion_strategy} {distortion_strategy}"
    
    def generate_response(self, history, current_input):
        formatted_history = "\n".join([
            f"{msg['role'].capitalize()}: {msg['message']}" for msg in history
        ])

        # SubLLM ë¶„ì„
        analysis = self.subllm.analyze(current_input)

        # ì „ëµ í”„ë¡¬í”„íŠ¸ ì¡°í•© (ìœ„ì¹˜ ê¸°ë°˜ ì¸ì ì „ë‹¬!)
        strategy_prompt = build_prompt_with_strategies(
            analysis["ë°˜ì‘ìœ í˜•"],
            analysis["ìƒë‹´ë‹¨ê³„"],
            analysis["ìƒë‹´ì ‘ê·¼ë²•"]
        )
        print("ğŸ”§ [ë””ë²„ê¹…] ìƒì„±ëœ strategy_prompt:")
        print(strategy_prompt)

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì±„ìš°ê¸°
        filled_prompt = self.prompt_template.format(
            persona_prompt=self.persona_prompt,
            client_info=self.client_info,
            history=formatted_history,
            current_input=current_input,
            emotion=analysis["ê°ì •"],
            distortion=analysis["ì¸ì§€ì™œê³¡"],
            strategy_prompt=strategy_prompt
        )

        # LLM í˜¸ì¶œ
        response = self.llm.invoke(filled_prompt)
        content = response.content if isinstance(response, AIMessage) else str(response)

        # ì‘ë‹µ ì¶”ì¶œ
        reply_match = re.search(r"ìƒë‹´ì‚¬\s*ì‘ë‹µ[:ï¼š]?\s*(.*?)(?=\n|$)", content, re.DOTALL)
        reply = reply_match.group(1).strip() if reply_match else content.strip()

        return {
            "reply": reply
        }

   